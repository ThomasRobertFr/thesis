\chapter{Conclusion}
\label{chapter:conclusion}

\acresetall
\acused{SHADE}

We first summarize the contributions that we propose in this thesis before discussing interesting directions for future work.

\section{Summary of Contributions}

In this thesis, we work on improving the quality of the latent spaces of deep \acfp{ConvNet} in three different contexts: regularization, \acf{SSL} and disentangling.

\paragraph{Regularizing through intra-class invariance.} Because powerful \acp{ConvNet} are easily subject to overfitting, we propose \ac{SHADE} in \autoref{chapter:shade}, a novel regularization method that encourages intra-class invariance in the representations. While the idea of adding invariance is not new, the novelty of our contribution lies in the way to enforce it. Indeed, we show that it is possible to use conditional entropy $\Ent(H\mid Y)$ as a measure of the intra-class invariance, which does not limit the types of variance that are targeted. We then derive a tractable and differentiable criterion from it. Applying this \ac{SHADE} criterion to many standard \ac{DNN} architectures, we are able to outperform other common forms of regularization and improve classification results on CIFAR-10 and ImageNet. We also demonstrate that \ac{SHADE} has the ability to significantly outperform the baseline on small datasets. This first contribution is a step in the direction of making \acp{ConvNet} usable in more practical cases, that we continue to explore in the following chapter with \acs{SSL}.


\paragraph{Information separation for \acf{SSL}.}
While \ac{SHADE} address the problem of scarce labeled data when overfitting becomes a preponderant issue, we then propose to use additional inexpensive unlabeled data to further make those models usable in more cases. We thus address \acf{SSL} in \autoref{chapter:hybridnet} to greatly improve the quality of a \ac{ConvNet}. In this context, we propose a two-branch architecture called HybridNet, accompanied by an adapted training loss, that separates the information in two complementary latent spaces. We show that this novel idea was able to efficiently address a conflict between classification and reconstruction that is detrimental to their cooperation, which is not the case with HybridNet. We demonstrate that HybridNet is able to integrate existing stability techniques and produce state-of-the-art results with large ResNet architectures on standard \ac{SSL} datasets, obtaining impressive performances with very few labeled samples. HybridNet is thus an interesting idea to make it possible to train large \acp{ConvNet} in contexts where labeled data is particularly expensive such as the medical domain.


\paragraph{Disentangling of two information domains.} Finally, pursuing our idea of separating the information in complementary latent spaces, we also address the problem of producing highly semantic disentangled representations in \autoref{chapter:dualdis}. To that end, we propose DualDis, an approach that allows to effectively separate and linearize two labeled and complementary information domains in two latent spaces. This study allows to underline some limitations of approaches using asymmetrical labeling of the domains (\ie only one information domain having labels) and shows that DualDis is able to produce interesting domain-specific representations that could then be used for other tasks such as information retrieval. Using the linearization properties of DualDis, it is possible to control the presence of the semantic attributes in the representation of an image, and thus perform semantic image editing. We show that this property can be leveraged for semantic data augmentation, producing new semantic variations of existing images.


\section{Perspectives for Future Work}

Let us now discuss interesting directions that could be addressed in future work in relation to our contributions.

\subsection*{Handling data from different domains with domain adaptation}

In this thesis, we had a particular focus on making \acp{ConvNet} usable in more practical cases by working on regularization and \ac{SSL}, reducing the need for large and expensive datasets. This goal can be pushed further by designing models that can handle data coming from different domains, which is called \textit{domain adaptation}.

With \textit{unsupervised} domain adaptation \citep{ben2010theory}, the goal is to transfer knowledge from a labeled source dataset to an unlabeled target dataset of interest, both sharing the same labels but having different image distributions. The goal being to leverage the knowledge of an existing or inexpensive (\eg synthetic) source dataset instead of getting expensive annotations for the target dataset. This problem is of particular interest nowadays for applications such as autonomous driving, where each city and country have different styles of cars, roadsigns, architecture, weather, \etc. To address domain adaptation, ideas from both HybridNet and DualDis could be used.

\paragraph{Domain adaptation through information separation.}
Indeed, a first possible solution for domain adaptation is to remove the information that is domain dependent and keep only the domain independent information. There is thus a clear link with the separation of discriminative and non-discriminative information in HybridNet. A solution in this direction has been proposed by \citet{Bousmalis2016a}, using three encoders, one that is common to both domains and should extract domain invariant features (used for classification), and one for each domain for domain specific features. The features are then merged and decoded for reconstruction. This shows that the idea of HybridNet to produce complementary reconstructions could be exploited further for domain adaptation. In addition to HybridNet, it would mostly require to design techniques to ensure the structuring of domain specific and domain agnostic information in the latent spaces, through adversarial training for example, and work on the architecture and merge strategy for this specific context.


\paragraph{Domain adaptation through generation.}
Another recent approach of domain adaptation consists in transforming the source images to target-like images, effectively removing the \textit{domain shift}. This allows to obtain a classifier adapted to the target domain, trained on those labeled target-like images. Following this idea, an extension of DualDis could help, offering the possibility of editing the image from one domain to the other. For example, \citet{chang2019all} recently proposed to disentangle domain invariant and domain specific features in order to perform this image editing. This approach is based on assumptions that the changes must be performed at the texture level without changing the structure of the image, which is not always the case. An extension of DualDis using some labels about domain specific factors could for example be developed to follow this idea.

\subsection*{Bridging the gap between discriminative and generative models}

An important motivation during this thesis was to develop models that could bridge the gap between discriminative and generative models. One the one hand, discriminative models aim at extracting discriminative and relevant patterns for a semantic task $\vy$ from input data $\vx$ (\ie model $p(\vy\mid \vx)$). On the other hand, recent generative models try to model the distribution of the data with no or small relation with semantic information (\ie model $p(\vx)$ or $p(\vx, \vy)$). With HybridNet and DualDis, we proposed models that are able to, at the same time, model all the information about the data and provide powerful discriminative and semantic features. However, some work still needs to be done to reach this goal. The objective would be to have a model that transforms an input $\vx$ into a representation $\vh$ in which many semantic factors $\vy$ can be analyzed, interpreted, manipulated; that can be decoded back into an image $\vxh$; and that also allows to generate new images $\vxt$ from scratch with fine control over the semantic factors. We propose to go over some possible directions for this.

\paragraph{\acf{SSL} of semantic factors.}
As studied in DualDis, we show that using labels related to semantic factors is important and has a great advantage over fully unsupervised models. However, to actually make those models usable in a large variety of cases, we believe that integrating \ac{SSL} in such models is a key direction. Indeed, the ideal situation would be to have a model that can learn to capture semantic factors of a dataset with only a few examples of each factor. This way, real world datasets that usually have many semantic factors could be semantically represented by this hybrid discriminative/generative model at a reasonable labeling expense. We showed preliminary results using \ac{SSL} in DualDis and we believe this should be further explored to make such a model viable.

\paragraph{Modeling factors diversity in separate subspaces.}
We also believe that it would be important to model the internal diversity of each semantic factor. Indeed, simple modelings of semantic factors consider they are represented by a binary value \citep[\eg\unskip][]{perarnau2016invertible} or a real value \citep[\eg\unskip][]{Lample2017}. 
Many semantic factors however have a large variability (\eg all the possible hairstyles, makeup styles, \etc that exist) that cannot be realistically represented in such limited spaces. With DualDis, we relieve this constraint and simply require that they be linearly separable, however, we cannot explore their internal variability. \citet{Klys2018} proposes the idea of representing each factors in delimited latent subspaces, but only show limited experiments on this idea. We thus believe this kind of work should be pursued.

\paragraph{Semantic generation and reversible architectures.}
Finally, the question of finding models that are both able to represent the information and produce sharp reconstructions and generations remains open. In DualDis, we did not address this problem, however, one goal of a good discriminative and generative model would be to eventually produce clear images \citep[\eg\unskip][]{karras2018style}. To achieve this, apart from the usual \acs{GAN} and perceptual losses that are known to help in this matter, reversible models \citep{NIPS2017_6816,jacobsen:hal-01712808} are also an interesting direction to address such an issue. Those models are, by design, able to encode and decode an image without loss of information. However, because of the particular layers they use to make this possible, manipulation in the latent information is made much more difficult. Some recent research shows promising results in this direction, such as \citet{kingma2018glow,lucas2019adversarial}. We believe this idea should be pursued and could be an important part of this goal of building a powerful model that could address both discriminative and generative tasks.
